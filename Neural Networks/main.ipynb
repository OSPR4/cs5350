{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "# import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Suppress RuntimeWarning\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "\n",
    "class Node():\n",
    "    def __init__(self):\n",
    "        self.next_layer_nodes = {}\n",
    "        self.inputs = {}\n",
    "        self.activation = 0\n",
    "        self.delta = 0\n",
    "    \n",
    "    def add_next_layer_node(self, node, weight):\n",
    "        self.next_layer_nodes[node] = weight\n",
    "\n",
    "    # def add\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def build_neural_network(layers, features, units, random_weights=True):\n",
    "    # layers: number of layers\n",
    "    # units: number of units in each layer\n",
    "    neural_network = {}\n",
    "    layer_node = []\n",
    "    nn_next_layer_nodes = []\n",
    "\n",
    "    for i in range(layers, -1, -1):\n",
    "        neural_network[i] = []\n",
    "        if i == layers:\n",
    "            output_node = Node()\n",
    "            neural_network[i].append(output_node)\n",
    "            nn_next_layer_nodes.append(output_node)\n",
    "        elif i > 0:\n",
    "            for j in range(units + 1):\n",
    "                node = Node()\n",
    "                neural_network[i].append(node)\n",
    "                for next_layer_node in nn_next_layer_nodes:\n",
    "                    w = 0\n",
    "                    if random_weights:\n",
    "                        w = np.random.normal(size=1)[0]\n",
    "                    node.next_layer_nodes[next_layer_node] = (w, 0)\n",
    "                if j == 0:\n",
    "                    node.activation = 1\n",
    "                    continue\n",
    "                layer_node.append(node)\n",
    "            nn_next_layer_nodes = []\n",
    "            for node in layer_node:\n",
    "                nn_next_layer_nodes.append(node)\n",
    "            layer_node.clear()\n",
    "        else:\n",
    "            for j in range(features + 1):\n",
    "                node = Node()\n",
    "                neural_network[i].append(node)\n",
    "                for next_layer_node in nn_next_layer_nodes:\n",
    "                    w = 0\n",
    "                    if random_weights:\n",
    "                        w = np.random.normal(size=1)[0]\n",
    "                    node.next_layer_nodes[next_layer_node] = (w, 0)\n",
    "                if j == 0:\n",
    "                    node.activation = 1\n",
    "    sorted_nn_dict = {k: neural_network[k] for k in sorted(neural_network)}\n",
    "\n",
    "    return sorted_nn_dict\n",
    "\n",
    "def forward_propagation(neural_network, inputs):\n",
    "    #append 1 to the beginning of the inputs\n",
    "    inputs = np.insert(inputs, 0, 1)\n",
    "\n",
    "    for i in range(len(inputs)):\n",
    "        neural_network[0][i].activation = inputs[i]\n",
    "\n",
    "    for layer_idx, layer in neural_network.items():\n",
    "        if layer_idx == 0:\n",
    "            # print(\"input layer to hidden layer\")\n",
    "            input_layer_to_hidden(layer)\n",
    "            # print(\"1::checking transfers to hidden layer\")\n",
    "            # for idx, node in enumerate(layer):\n",
    "            #     print(\"     node: \", idx)\n",
    "            #     for nl_node in node.next_layer_nodes:\n",
    "            #         print(\"     next layer node: \", nl_node)\n",
    "            #         print(\"     next layer node input at index: \", idx, \" is: \", nl_node.inputs[idx])\n",
    "        elif layer_idx == len(neural_network) - 1:\n",
    "            # print(\"hidden to output layer\")\n",
    "            hidden_layer_to_output(layer)\n",
    "        else:\n",
    "            # print(\"hidden to hidden layer\")\n",
    "            hidden_layer_to_hidden(layer)\n",
    "    # output = neural_network[len(neural_network) - 1][0].activation\n",
    "    # print(output)\n",
    "    return neural_network\n",
    "\n",
    "def input_layer_to_hidden(layer):\n",
    "\n",
    "    for idx, node in enumerate(layer):\n",
    "\n",
    "        for nl_node in node.next_layer_nodes:\n",
    "            input = node.activation\n",
    "            weight = node.next_layer_nodes[nl_node][0]\n",
    "            output = input * weight\n",
    "            nl_node.inputs[idx] = output\n",
    "            nl_node.activation = output\n",
    "\n",
    "def hidden_layer_to_hidden(layer):\n",
    "    for idx, node in enumerate(layer):\n",
    "        # print(\"     node: \", idx)\n",
    "        activation = 0\n",
    "        if idx != 0:\n",
    "            sigmoid_input = 0\n",
    "            for i in node.inputs:\n",
    "                sigmoid_input += node.inputs[i]\n",
    "            # print(\"     sigmoid_input: \", sigmoid_input)\n",
    "            activation = sigmoid(sigmoid_input)\n",
    "        else:\n",
    "            activation = 1\n",
    "        node.activation = activation\n",
    "        # print(\"     activation: \", activation)\n",
    "\n",
    "        for nl_node in node.next_layer_nodes:\n",
    "            w = node.next_layer_nodes[nl_node][0]\n",
    "            output = activation * w\n",
    "            nl_node.inputs[idx] = output\n",
    "\n",
    "def hidden_layer_to_output(layer):\n",
    "    node = layer[0]\n",
    "    output = 0\n",
    "    for i in node.inputs:\n",
    "        output += node.inputs[i]\n",
    "    node.activation = output\n",
    "    # print(\"output\", output)\n",
    "\n",
    "def back_propagation_deltas(neural_network, target):\n",
    "    for layer_idx in range(len(neural_network) - 1, -1, -1):\n",
    "        # print(\"layer: \", layer_idx)\n",
    "        layer = neural_network[layer_idx]\n",
    "        if layer_idx == len(neural_network) - 1:\n",
    "            output_node = layer[0]\n",
    "            output_node.delta = output_node.activation - target\n",
    "            # print(\"output_node.delta: \", output_node.delta)\n",
    "        elif layer_idx == 0:\n",
    "            pass\n",
    "        else:\n",
    "            for node in layer:\n",
    "                delta = 0\n",
    "                for nl_node in node.next_layer_nodes:\n",
    "                    nl_node_delta = nl_node.delta\n",
    "                    w = node.next_layer_nodes[nl_node][0]\n",
    "                    delta += nl_node_delta * w * node.activation * (1 - node.activation)\n",
    "                node.delta = delta\n",
    "                # print(\"node.delta: \", node.delta)\n",
    "    return neural_network\n",
    "                    \n",
    "def compute_gradients(neural_network):\n",
    "    for layer_idx, layer in neural_network.items():\n",
    "        # print(\"layer: \", layer_idx)\n",
    "        if layer_idx == len(neural_network) - 1:\n",
    "            continue\n",
    "        else:\n",
    "            for idx, node in enumerate(layer):\n",
    "                # print(\"node: \", idx)\n",
    "                for nl_node in node.next_layer_nodes:\n",
    "                    nl_node_delta = nl_node.delta\n",
    "                    partial_derivative = nl_node_delta * node.activation\n",
    "                    # print(\"partial_derivative: \", partial_derivative)\n",
    "                    node.next_layer_nodes[nl_node] = (node.next_layer_nodes[nl_node][0], partial_derivative)\n",
    "    return neural_network\n",
    "\n",
    "def update_weights(neural_network, learning_rate):\n",
    "    for layer_idx, layer in neural_network.items():\n",
    "        if layer_idx == len(neural_network) - 1:\n",
    "            continue\n",
    "        else:\n",
    "            for node in layer:\n",
    "                for nl_node in node.next_layer_nodes:\n",
    "                    w = node.next_layer_nodes[nl_node][0]\n",
    "                    partial_derivative = node.next_layer_nodes[nl_node][1]\n",
    "                    w = w - learning_rate * partial_derivative\n",
    "                    node.next_layer_nodes[nl_node] = (w, partial_derivative)\n",
    "    return neural_network\n",
    "        \n",
    "def compute_error(neural_network, X, y):\n",
    "    error = 0\n",
    "    for i in range(len(X)):\n",
    "        neural_network = forward_propagation(neural_network, X[i])\n",
    "        output_node = neural_network[len(neural_network) - 1][0]\n",
    "        error += (output_node.activation - y[i]) ** 2\n",
    "    return error / len(X)\n",
    "\n",
    "\n",
    "\n",
    "def train_neural_network(neural_network, X, y, learning_rate, d, epochs, tolerance=0.001):\n",
    "    error = compute_error(neural_network, X, y)\n",
    "    convergence = False\n",
    "    iterations = 0\n",
    "    objective_function_values = []\n",
    "    objective_function_values.append(error)\n",
    "    # print(\"initial error: \", error)\n",
    "\n",
    "    while not convergence and iterations < epochs:\n",
    "        # shuffle the data\n",
    "        shuffle_indices = np.random.permutation(len(X))\n",
    "        X = X[shuffle_indices]\n",
    "        y = y[shuffle_indices]\n",
    "        for j in range(len(X)):\n",
    "            neural_network = forward_propagation(neural_network, X[j])\n",
    "            neural_network = back_propagation_deltas(neural_network, y[j])\n",
    "            neural_network = compute_gradients(neural_network)\n",
    "            neural_network = update_weights(neural_network, learning_rate)\n",
    "        current_error = compute_error(neural_network, X, y)\n",
    "        # print(\"epoch: \", iterations, \" error: \", current_error)\n",
    "        error_difference = abs(current_error - error)\n",
    "        if error_difference < tolerance:\n",
    "            break\n",
    "        error = current_error\n",
    "        objective_function_values.append(error)\n",
    "        iterations += 1\n",
    "    # plt.plot(objective_function_values)\n",
    "    # plt.ylabel('objective function value')\n",
    "    # plt.xlabel('epoch')\n",
    "    # plt.show()\n",
    "    return neural_network\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def predict(neural_network, X):\n",
    "    predictions = []\n",
    "    for i in range(len(X)):\n",
    "        neural_network = forward_propagation(neural_network, X[i])\n",
    "        output_node = neural_network[len(neural_network) - 1][0]\n",
    "        predictions.append(output_node.activation)\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural_network_test = build_neural_network(layers=3, features=2, units=2)\n",
    "\n",
    "# layer_0_weights = {0: [-1, 1], 1: [-2, 2], 2: [-3, 3]}\n",
    "# layer_1_weights = {0: [-1, 1], 1: [-2, 2], 2: [-3, 3]}\n",
    "# layer_2_weights = {0: [-1], 1: [2], 2: [-1.5]}\n",
    "\n",
    "# new_weights = {0: layer_0_weights, 1: layer_1_weights, 2: layer_2_weights}\n",
    "\n",
    "\n",
    "# for layer_idx, layer in neural_network_test.items():   \n",
    "#     for node_idx, node in enumerate(layer):\n",
    "#         i = 0\n",
    "#         for nl_node_idx, nl_node in node.next_layer_nodes.items():\n",
    "#             new_w = new_weights[layer_idx][node_idx][i]\n",
    "#             node.next_layer_nodes[nl_node_idx] = (new_w, 0)\n",
    "#             i += 1\n",
    "\n",
    "# # for layer_idx, layer in neural_network_test.items():\n",
    "# #     print(\"layer: \", layer_idx)\n",
    "# #     for node_idx, node in enumerate(layer):\n",
    "# #         print(\"weights\")\n",
    "# #         print(\"node: \", node_idx)\n",
    "# #         for nl_node in node.next_layer_nodes:\n",
    "# #             print(node.next_layer_nodes[nl_node][0])\n",
    "# x = np.array([[1, 1]])\n",
    "# y = np.array([-1])\n",
    "\n",
    "# print(\"len of x: \", len(x[0])) \n",
    "\n",
    "# neural_network_test = train_neural_network(neural_network_test, x, y, learning_rate=0.1, d=1, epochs=100, tolerance=0.00001)\n",
    "# predictions = predict(neural_network_test, x)\n",
    "# print(predictions)\n",
    "\n",
    "\n",
    "# # neural_network_test = forward_propagation(neural_network_test, x)\n",
    "# # output = neural_network_test[len(neural_network_test) - 1][0].activation\n",
    "# # print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09182\n",
      "0.0002267954\n",
      "0.09161358864\n"
     ]
    }
   ],
   "source": [
    "delta = 0.09182\n",
    "z =[1, 0.00247, 0.997752]\n",
    "for i in range(len(z)):\n",
    "    partial_derivative = delta * z[i]\n",
    "    print(partial_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) [17 points] Implement the stochastic gradient descent algorithm to learn the neu-\n",
    "ral netowrk from the training data. Use the schedule of learning rate: γt = γ0\n",
    "1+γ0d t.\n",
    "Initialize the edge weights with random numbers generated from the standard\n",
    "Gaussian distribution. We restrict the width, i.e., the number of nodes, of\n",
    "each hidden layer (i.e., Layer 1 & 2 ) to be identical. Vary the width from\n",
    "{5,10,25,50,100}. Please tune γ0 and d to ensure convergence. Use the curve\n",
    "of the objective function (along with the number of updates) to diagnosis the\n",
    "convergence. Don’t forget to shuffle the training examples at the start of each\n",
    "epoch. Report the training and test error for each setting of the width.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width:  5\n",
      "train error:  0.009174311926605505\n",
      "test error:  0.012\n",
      "width:  10\n",
      "train error:  0.04472477064220184\n",
      "test error:  0.052\n",
      "width:  25\n",
      "train error:  0.052752293577981654\n",
      "test error:  0.058\n",
      "width:  50\n",
      "train error:  0.02981651376146789\n",
      "test error:  0.028\n",
      "width:  100\n",
      "train error:  0.4461009174311927\n",
      "test error:  0.442\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_training = np.genfromtxt('../data/bank-note/train.csv', delimiter=',')\n",
    "X_test = np.genfromtxt('../data/bank-note/test.csv', delimiter=',')\n",
    "\n",
    "y_training = X_training[:, -1]\n",
    "y_test = X_test[:, -1]\n",
    "\n",
    "y_training[y_training == 0] = -1\n",
    "y_test[y_test == 0] = -1\n",
    "\n",
    "# X_training = np.insert(X_training, 0, 1, axis=1)\n",
    "# X_test = np.insert(X_test, 0, 1, axis=1)\n",
    "\n",
    "X_training = X_training[:, :-1]\n",
    "X_test = X_test[:, :-1]\n",
    "\n",
    "units = [5,10,25,50,100]\n",
    "layers = 3\n",
    "features = 4\n",
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "tolerance = 0.0001\n",
    "d=1\n",
    "\n",
    "for unit in units:\n",
    "    neural_network = build_neural_network(layers=layers, features=features, units=unit, random_weights=True)\n",
    "    train_neural_network(neural_network, X_training, y_training, learning_rate=learning_rate, d=d, epochs=epochs, tolerance=tolerance)\n",
    "    train_predictions = predict(neural_network, X_training)\n",
    "    test_predictions = predict(neural_network, X_test)\n",
    "    train_error = 0\n",
    "    test_error = 0\n",
    "    for i in range(len(train_predictions)):\n",
    "        if train_predictions[i] > 0.9:\n",
    "            train_predictions[i] = 1\n",
    "        else:\n",
    "            train_predictions[i] = -1\n",
    "        if train_predictions[i] != y_training[i]:\n",
    "            train_error += 1\n",
    "    for i in range(len(test_predictions)):\n",
    "        if test_predictions[i] > 0.9:\n",
    "            test_predictions[i] = 1\n",
    "        else:\n",
    "            test_predictions[i] = -1\n",
    "        if test_predictions[i] != y_test[i]:\n",
    "            test_error += 1\n",
    "    print(\"width: \", unit)\n",
    "    print(\"train error: \", train_error / len(train_predictions))\n",
    "    print(\"test error: \", test_error / len(test_predictions))\n",
    "    # print(\"train predictions: \", train_predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) [10 points]. Now initialize all the weights with 0, and run your training algorithm\n",
    "again. What is your training and test error? What do you observe and conclude?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "width:  5\n",
      "train error:  0.04472477064220184\n",
      "test error:  0.062\n",
      "width:  10\n",
      "train error:  0.14564220183486237\n",
      "test error:  0.166\n",
      "width:  25\n",
      "train error:  0.1926605504587156\n",
      "test error:  0.206\n",
      "width:  50\n",
      "train error:  0.1651376146788991\n",
      "test error:  0.186\n",
      "width:  100\n",
      "train error:  0.34059633027522934\n",
      "test error:  0.338\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_training = np.genfromtxt('../data/bank-note/train.csv', delimiter=',')\n",
    "X_test = np.genfromtxt('../data/bank-note/test.csv', delimiter=',')\n",
    "\n",
    "y_training = X_training[:, -1]\n",
    "y_test = X_test[:, -1]\n",
    "\n",
    "y_training[y_training == 0] = -1\n",
    "y_test[y_test == 0] = -1\n",
    "\n",
    "\n",
    "X_training = X_training[:, :-1]\n",
    "X_test = X_test[:, :-1]\n",
    "\n",
    "units = [5,10,25,50,100]\n",
    "layers = 3\n",
    "features = 4\n",
    "learning_rate = 0.1\n",
    "epochs = 10\n",
    "tolerance = 0.0001\n",
    "d=1\n",
    "\n",
    "for unit in units:\n",
    "    neural_network = build_neural_network(layers=layers, features=features, units=unit, random_weights=False)\n",
    "    train_neural_network(neural_network, X_training, y_training, learning_rate=learning_rate, d=d, epochs=epochs, tolerance=tolerance)\n",
    "    train_predictions = predict(neural_network, X_training)\n",
    "    test_predictions = predict(neural_network, X_test)\n",
    "    train_error = 0\n",
    "    test_error = 0\n",
    "    for i in range(len(train_predictions)):\n",
    "        if train_predictions[i] > 0.9:\n",
    "            train_predictions[i] = 1\n",
    "        else:\n",
    "            train_predictions[i] = -1\n",
    "        if train_predictions[i] != y_training[i]:\n",
    "            train_error += 1\n",
    "    for i in range(len(test_predictions)):\n",
    "        if test_predictions[i] > 0.9:\n",
    "            test_predictions[i] = 1\n",
    "        else:\n",
    "            test_predictions[i] = -1\n",
    "        if test_predictions[i] != y_test[i]:\n",
    "            test_error += 1\n",
    "    print(\"width: \", unit)\n",
    "    print(\"train error: \", train_error / len(train_predictions))\n",
    "    print(\"test error: \", test_error / len(test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) [6 points]. As compared with the performance of SVM (and the logistic regression\n",
    "you chose to implement it; see Problem 3), what do you conclude (empirically)\n",
    "about the neural network?\n",
    "\n",
    "The performance of the neural network was very similar to the SVM, but the seems to have performed better if the right about of nodes in the hidden layer was chosen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[30 points] Please use PyTorch (or TensorFlow if you want) to fulfill\n",
    "the neural network training and prediction. Please try two activation functions,\n",
    "“tanh” and “RELU”. For “tanh”, please use the “Xavier’ initialization; and for\n",
    "“RELU”, please use the “he” initialization. You can implement these initializa-\n",
    "tions by yourselves or use PyTorch (or TensorFlow) library. Vary the depth from\n",
    "{3,5,9} and width from {5,10,25,50,100}. Pleas use the Adam optimizer for\n",
    "training. The default settings of Adam should be sufficient (e.g., initial learning\n",
    "rate is set to 10−3). Report the training and test error with each (depth, width)\n",
    "combination. What do you observe and conclude? Note that, we won’t provide\n",
    "any link or manual for you to work on this bonus problem. It is YOUR JOB\n",
    "to search the documentation, find code snippets, test, and debug with PyTorch\n",
    "(or TensorFlow) to ensure the correct usage. This is what all machine learning\n",
    "practitioners do in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " install PyTorch in a specific Python environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error\n",
      "Depth: 3, Width: 5, Activation: tanh, Error: 0.5580000281333923\n",
      "Train Error\n",
      "Depth: 3, Width: 5, Activation: tanh, Error: 0.5538991093635559\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 3, Width: 5, Activation: relu, Error: 0.5580000281333923\n",
      "Train Error\n",
      "Depth: 3, Width: 5, Activation: relu, Error: 0.5538991093635559\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 3, Width: 10, Activation: tanh, Error: 0.5580000281333923\n",
      "Train Error\n",
      "Depth: 3, Width: 10, Activation: tanh, Error: 0.5538991093635559\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 3, Width: 10, Activation: relu, Error: 0.5580000281333923\n",
      "Train Error\n",
      "Depth: 3, Width: 10, Activation: relu, Error: 0.5538991093635559\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 3, Width: 25, Activation: tanh, Error: 0.2800000011920929\n",
      "Train Error\n",
      "Depth: 3, Width: 25, Activation: tanh, Error: 0.2958715558052063\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 3, Width: 25, Activation: relu, Error: 0.49399998784065247\n",
      "Train Error\n",
      "Depth: 3, Width: 25, Activation: relu, Error: 0.5034403800964355\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 3, Width: 50, Activation: tanh, Error: 0.23800000548362732\n",
      "Train Error\n",
      "Depth: 3, Width: 50, Activation: tanh, Error: 0.25\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 3, Width: 50, Activation: relu, Error: 0.44200000166893005\n",
      "Train Error\n",
      "Depth: 3, Width: 50, Activation: relu, Error: 0.4461009204387665\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 3, Width: 100, Activation: tanh, Error: 0.03799999877810478\n",
      "Train Error\n",
      "Depth: 3, Width: 100, Activation: tanh, Error: 0.04472476989030838\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 3, Width: 100, Activation: relu, Error: 0.25600001215934753\n",
      "Train Error\n",
      "Depth: 3, Width: 100, Activation: relu, Error: 0.25344038009643555\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 5, Width: 5, Activation: tanh, Error: 0.44200000166893005\n",
      "Train Error\n",
      "Depth: 5, Width: 5, Activation: tanh, Error: 0.4461009204387665\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 5, Width: 5, Activation: relu, Error: 0.5580000281333923\n",
      "Train Error\n",
      "Depth: 5, Width: 5, Activation: relu, Error: 0.5538991093635559\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 5, Width: 10, Activation: tanh, Error: 0.5580000281333923\n",
      "Train Error\n",
      "Depth: 5, Width: 10, Activation: tanh, Error: 0.5538991093635559\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 5, Width: 10, Activation: relu, Error: 0.33000001311302185\n",
      "Train Error\n",
      "Depth: 5, Width: 10, Activation: relu, Error: 0.33256879448890686\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 5, Width: 25, Activation: tanh, Error: 0.44200000166893005\n",
      "Train Error\n",
      "Depth: 5, Width: 25, Activation: tanh, Error: 0.4461009204387665\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 5, Width: 25, Activation: relu, Error: 0.4659999907016754\n",
      "Train Error\n",
      "Depth: 5, Width: 25, Activation: relu, Error: 0.427752286195755\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 5, Width: 50, Activation: tanh, Error: 0.10999999940395355\n",
      "Train Error\n",
      "Depth: 5, Width: 50, Activation: tanh, Error: 0.11238531768321991\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 5, Width: 50, Activation: relu, Error: 0.02800000086426735\n",
      "Train Error\n",
      "Depth: 5, Width: 50, Activation: relu, Error: 0.03211009129881859\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 5, Width: 100, Activation: tanh, Error: 0.3179999887943268\n",
      "Train Error\n",
      "Depth: 5, Width: 100, Activation: tanh, Error: 0.3417431116104126\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 5, Width: 100, Activation: relu, Error: 0.1899999976158142\n",
      "Train Error\n",
      "Depth: 5, Width: 100, Activation: relu, Error: 0.17431192100048065\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 9, Width: 5, Activation: tanh, Error: 0.5580000281333923\n",
      "Train Error\n",
      "Depth: 9, Width: 5, Activation: tanh, Error: 0.5538991093635559\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 9, Width: 5, Activation: relu, Error: 0.5580000281333923\n",
      "Train Error\n",
      "Depth: 9, Width: 5, Activation: relu, Error: 0.5538991093635559\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 9, Width: 10, Activation: tanh, Error: 0.5580000281333923\n",
      "Train Error\n",
      "Depth: 9, Width: 10, Activation: tanh, Error: 0.5538991093635559\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 9, Width: 10, Activation: relu, Error: 0.44200000166893005\n",
      "Train Error\n",
      "Depth: 9, Width: 10, Activation: relu, Error: 0.4461009204387665\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 9, Width: 25, Activation: tanh, Error: 0.5580000281333923\n",
      "Train Error\n",
      "Depth: 9, Width: 25, Activation: tanh, Error: 0.5538991093635559\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 9, Width: 25, Activation: relu, Error: 0.10199999809265137\n",
      "Train Error\n",
      "Depth: 9, Width: 25, Activation: relu, Error: 0.09747706353664398\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 9, Width: 50, Activation: tanh, Error: 0.44200000166893005\n",
      "Train Error\n",
      "Depth: 9, Width: 50, Activation: tanh, Error: 0.4461009204387665\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 9, Width: 50, Activation: relu, Error: 0.44200000166893005\n",
      "Train Error\n",
      "Depth: 9, Width: 50, Activation: relu, Error: 0.4461009204387665\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 9, Width: 100, Activation: tanh, Error: 0.3100000023841858\n",
      "Train Error\n",
      "Depth: 9, Width: 100, Activation: tanh, Error: 0.33256879448890686\n",
      "\n",
      "\n",
      "Test Error\n",
      "Depth: 9, Width: 100, Activation: relu, Error: 0.44200000166893005\n",
      "Train Error\n",
      "Depth: 9, Width: 100, Activation: relu, Error: 0.4461009204387665\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    " \n",
    "class Network(nn.Module):\n",
    "    def __init__(self, depth, width, input_dim, output_dim, activation):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activation = None\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                self.layers.append(nn.Linear(input_dim, width))\n",
    "            else:\n",
    "                self.layers.append(nn.Linear(width, width))\n",
    "        self.layers.append(nn.Linear(width, output_dim))\n",
    "        \n",
    "        if activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "            for layer in self.layers:\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.ones_(layer.bias)\n",
    "        elif activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "            for layer in self.layers:\n",
    "                nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "                nn.init.ones_(layer.bias)\n",
    "        else:\n",
    "            raise ValueError('Invalid activation function')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = self.activation(layer(x))\n",
    "            # if layer != self.layers[-1]:\n",
    "            # x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "X_training = np.genfromtxt('../data/bank-note/train.csv', delimiter=',')\n",
    "X_test = np.genfromtxt('../data/bank-note/test.csv', delimiter=',')\n",
    "\n",
    "y_training = X_training[:, -1]\n",
    "y_test = X_test[:, -1]\n",
    "\n",
    "\n",
    "# X_training = np.insert(X_training, 0, 1, axis=1)\n",
    "# X_test = np.insert(X_test, 0, 1, axis=1)\n",
    "\n",
    "X_training = X_training[:, :-1]\n",
    "X_test = X_test[:, :-1]\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_training_torch = torch.from_numpy(X_training).float()\n",
    "y_training_torch = torch.from_numpy(y_training).float().view(-1, 1)\n",
    "X_test_torch = torch.from_numpy(X_test).float()\n",
    "y_test_torch = torch.from_numpy(y_test).float().view(-1, 1)\n",
    "\n",
    "depths = [3,5,9]\n",
    "widths = [5,10,25, 50, 100]\n",
    "activations = ['tanh', 'relu']\n",
    "\n",
    "\n",
    "for depth in depths:\n",
    "    for width in widths:\n",
    "        for activation in activations:\n",
    "            model = Network(depth, width, 4, 1, activation)\n",
    "            optimizer = optim.Adam(model.parameters())\n",
    "            loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "            for epoch in range(10): \n",
    "                optimizer.zero_grad()\n",
    "                output = model(X_training_torch)\n",
    "                loss = loss_fn(output, y_training_torch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                print(\"Test Error\")\n",
    "                output_test = model(X_test_torch)\n",
    "                predictions_test = torch.round(torch.sigmoid(output_test))\n",
    "                error_test = (predictions_test != y_test_torch).float().mean()\n",
    "                print(f'Depth: {depth}, Width: {width}, Activation: {activation}, Error: {error_test.item()}')\n",
    "\n",
    "                print(\"Train Error\")\n",
    "                output_training = model(X_training_torch)\n",
    "                predictions_training = torch.round(torch.sigmoid(output_training))\n",
    "                error_training = (predictions_training != y_training_torch).float().mean()\n",
    "                print(f'Depth: {depth}, Width: {width}, Activation: {activation}, Error: {error_training.item()}')\n",
    "                print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Bonus] [30 points] We will implement the logistic regression model with stochastic\n",
    "gradient descent. We will use the dataset “bank-note.zip” in Canvas. Set the maximum\n",
    "number of epochs T to 100. Don’t forget to shuffle the training examples at the start of\n",
    "each epoch. Use the curve of the objective function (along with the number of updates)\n",
    "to diagnosis the convergence. We initialize all the model parameters with 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
