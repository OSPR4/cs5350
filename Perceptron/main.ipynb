{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from perceptron import Perceptron\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement Perceptron for a binary classification task — bank-note authen-\n",
    "tication. Please download the data “bank-note.zip” from Canvas. The features and\n",
    "labels are listed in the file “bank-note/data-desc.txt”. The training data are stored in\n",
    "the file “bank-note/train.csv”, consisting of 872 examples. The test data are stored in\n",
    "“bank-note/test.csv”, and comprise of 500 examples. In both the training and testing\n",
    "datasets, feature values and labels are separated by commas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) [16 points] Implement the standard Perceptron. Set the maximum number of\n",
    "epochs T to 10. Report your learned weight vector, and the average prediction\n",
    "error on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Vector: [ -0.94       -17.86444252 -24.46525945   3.94475322   1.01399766]\n",
      "Standard Perceptron Error: 0.312\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from perceptron import Perceptron\n",
    "import numpy as np\n",
    "\n",
    "X_training = np.genfromtxt('../data/bank-note/train.csv', delimiter=',')\n",
    "X_test = np.genfromtxt('../data/bank-note/test.csv', delimiter=',')\n",
    "y_training = X_training[:, -1]\n",
    "y_test = X_test[:, -1]\n",
    "y_training[y_training == 0] = -1\n",
    "y_test[y_test == 0] = -1\n",
    "\n",
    "X_training = np.insert(X_training, 0, 1, axis=1)\n",
    "X_test = np.insert(X_test, 0, 1, axis=1)\n",
    "\n",
    "X_training = X_training[:, :-1]\n",
    "X_test = X_test[:, :-1]\n",
    "\n",
    "clf = Perceptron(variant='standard', epoch=1, r=0.01)\n",
    "clf.fit(X_training, y_training)\n",
    "y_predicted = clf.predict(X_test)\n",
    "\n",
    "error = np.sum(y_predicted != y_test) / len(y_test)\n",
    "print(\"Weight Vector: \" + str(clf.w))\n",
    "print(\"Standard Perceptron Error: \" + str(error))\n",
    "\n",
    "\n",
    "# avg_error = 0\n",
    "# for i in range(0, 500):\n",
    "#     clf = Perceptron(variant='standard', epoch=10, r=0.01)\n",
    "#     clf.fit(X_training, y_training)\n",
    "#     y_predicted = clf.predict(X_test)\n",
    "\n",
    "#     error = 0\n",
    "#     for i in range(0, len(X_test)):\n",
    "#         if y_predicted[i] != y_test[i]:\n",
    "#             error += 1\n",
    "#     error = error / len(X_test)\n",
    "#     avg_error += error\n",
    "# avg_error = avg_error / 500\n",
    "# print(\"Weight Vector: \" + str(clf.w))\n",
    "# print(\"Standard Perceptron Error: \" + str(avg_error))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W: [ 0.59       -0.61554356 -0.41836538 -0.37473877 -0.07858299]\n",
    "train_error: 0.020642201834862386\n",
    "test_error: 0.016\n",
    "MSE: 0.064\n",
    "\n",
    "W: [ 0.56       -0.6225498  -0.45550499 -0.2917123  -0.09865261]\n",
    "train_error: 0.0389908256880734\n",
    "test_error: 0.06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) [16 points] Implement the voted Perceptron. Set the maximum number of epochs\n",
    "T to 10. Report the list of the distinct weight vectors and their counts — the\n",
    "number of correctly predicted training examples. Using this set of weight vectors\n",
    "to predict each test example. Report the average test error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voted Perceptron Error: 0.312\n"
     ]
    }
   ],
   "source": [
    "X_training = np.genfromtxt('../data/bank-note/train.csv', delimiter=',')\n",
    "X_test = np.genfromtxt('../data/bank-note/test.csv', delimiter=',')\n",
    "y_training = X_training[:, -1]\n",
    "y_test = X_test[:, -1]\n",
    "y_training[y_training == 0] = -1\n",
    "y_test[y_test == 0] = -1\n",
    "\n",
    "X_training = np.insert(X_training, 0, 1, axis=1)\n",
    "X_test = np.insert(X_test, 0, 1, axis=1)\n",
    "\n",
    "X_training = X_training[:, :-1]\n",
    "X_test = X_test[:, :-1]\n",
    "\n",
    "clf = Perceptron(variant='voted', epoch=10, r=0.01)\n",
    "clf.fit(X_training, y_training)\n",
    "y_predicted = clf.predict(X_test)\n",
    "\n",
    "error = 0\n",
    "for i in range(0, len(X_test)):\n",
    "    if y_predicted[i] != y_test[i]:\n",
    "        error += 1\n",
    "error = error / len(X_test)\n",
    "print(\"Voted Perceptron Error: \" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:   0, W: [0. 0. 0. 0. 0.]\n",
      "c:   2, W: [-0.01     -0.038481 -0.101539  0.038561  0.042228]\n",
      "c:   1, W: [-0.02       -0.03800092 -0.085502   -0.046195    0.0346722 ]\n",
      "c:   8, W: [-0.01       -0.05066792 -0.057319   -0.070455    0.0158102 ]\n",
      "c:   3, W: [ 0.         -0.07119692 -0.018934   -0.0784094   0.0036722 ]\n",
      "c:   7, W: [ 0.01       -0.10388892 -0.14634     0.0771636   0.002254  ]\n",
      "c:   5, W: [ 0.02       -0.14309292 -0.105617    0.0747958  -0.018897  ]\n",
      "c:   9, W: [ 0.01       -0.17775992 -0.064893    0.0319138  -0.034315  ]\n",
      "c:  10, W: [ 0.         -0.17658162 -0.049104   -0.0483862  -0.03403469]\n",
      "c:   2, W: [ 0.01       -0.16208062 -0.013037   -0.0889432  -0.05000069]\n",
      "c:   1, W: [ 0.02       -0.20884562 -0.069673    0.0207468  -0.05334559]\n",
      "c:   3, W: [ 0.01       -0.18236662 -0.171047    0.0340568   0.00136141]\n",
      "c:   1, W: [ 0.02       -0.18120742 -0.138828   -0.0002452  -0.02709559]\n",
      "c:   8, W: [ 0.03       -0.20708742 -0.100174   -0.0035812  -0.03989259]\n",
      "c:   2, W: [ 0.04       -0.20587482 -0.0979393  -0.0083139  -0.03019019]\n",
      "c:  10, W: [ 0.05       -0.21078292 -0.0694873  -0.0447499  -0.06119419]\n",
      "c:   2, W: [ 0.06       -0.18883992 -0.0239843  -0.0945099  -0.08844819]\n",
      "c:   3, W: [ 0.07       -0.22905792 -0.1070243   0.0310401  -0.10354719]\n",
      "c:  14, W: [ 0.08       -0.22795472 -0.0872833  -0.0026279  -0.11007309]\n",
      "c:   6, W: [ 0.09       -0.22494662 -0.0855452  -0.0201699  -0.10518099]\n",
      "c:  11, W: [ 0.1        -0.21894162 -0.0662182  -0.0530579  -0.10842249]\n",
      "c:  17, W: [ 0.11       -0.20838962 -0.0543612  -0.0794689  -0.10731919]\n",
      "c:  27, W: [ 0.1        -0.20890941 -0.1248822  -0.0589279  -0.07581119]\n",
      "c:  30, W: [ 0.09       -0.22034141 -0.0874692  -0.1147049  -0.06945339]\n",
      "c:   3, W: [ 0.08       -0.19386241 -0.1888432  -0.1013949  -0.01474639]\n",
      "c:  12, W: [ 0.09       -0.18419161 -0.1504172  -0.1507089  -0.05606939]\n",
      "c:   6, W: [ 0.08       -0.18748401 -0.1058652  -0.1964269  -0.04618139]\n",
      "c:   7, W: [ 0.09       -0.20458801 -0.1536452  -0.1343179  -0.04220739]\n",
      "c:  31, W: [ 0.1        -0.19899411 -0.1567492  -0.1324872  -0.03774209]\n",
      "c:  13, W: [ 0.09       -0.20418911 -0.1241162  -0.1633822  -0.02789309]\n",
      "c:   2, W: [ 0.1        -0.23561211 -0.2544812  -0.0066092  -0.03450959]\n",
      "c:   1, W: [ 0.09       -0.25286911 -0.2097842  -0.0888282  -0.01643659]\n",
      "c:  30, W: [ 0.1        -0.32329011 -0.1177842  -0.0862349  -0.06326859]\n",
      "c:  53, W: [ 0.11       -0.30649111 -0.0757162  -0.1316329  -0.08719959]\n",
      "c:  15, W: [ 0.12       -0.29297311 -0.0651212  -0.1550699  -0.08319979]\n",
      "c:  24, W: [ 0.13       -0.32803311 -0.1907882  -0.0034639  -0.09072139]\n",
      "c:  18, W: [ 0.14       -0.31070211 -0.1512442  -0.0508759  -0.11573839]\n",
      "c:  17, W: [ 0.13       -0.32891811 -0.0864962  -0.1313899  -0.11155289]\n",
      "c:   5, W: [ 0.12       -0.32104911 -0.1821592  -0.0935229  -0.03651889]\n",
      "c:  18, W: [ 0.13       -0.30597211 -0.1625632  -0.1241069  -0.03774319]\n",
      "c: 104, W: [ 0.14       -0.29585511 -0.1535412  -0.1476129  -0.03347179]\n",
      "c:  21, W: [ 0.15       -0.27193811 -0.1079762  -0.1975009  -0.06245879]\n",
      "c:  27, W: [ 0.16       -0.30799111 -0.1677162  -0.0965849  -0.07074339]\n",
      "c:  27, W: [ 0.15       -0.30638031 -0.1030922  -0.1801579  -0.05552739]\n",
      "c:  12, W: [ 0.16       -0.34334131 -0.2398712  -0.0043629  -0.08170839]\n",
      "c:   1, W: [ 0.17       -0.33439011 -0.1921332  -0.0527939  -0.13761739]\n",
      "c: 113, W: [ 0.16       -0.33958481 -0.1595002  -0.0836889  -0.12776819]\n",
      "c:   1, W: [ 0.15       -0.34287681 -0.1149482  -0.1294069  -0.11788019]\n",
      "c:  10, W: [ 0.16       -0.32256681 -0.0964282  -0.1595279  -0.11785016]\n",
      "c:  28, W: [ 0.15       -0.31893891 -0.1793232  -0.1403149  -0.08451816]\n",
      "c:  48, W: [ 0.16       -0.29876191 -0.1613412  -0.1698959  -0.08241916]\n",
      "c:   7, W: [ 0.17       -0.28656391 -0.1403592  -0.2018499  -0.08113486]\n",
      "c:  29, W: [ 0.16       -0.27869491 -0.2360222  -0.1639829  -0.00610086]\n",
      "c:  24, W: [ 0.17       -0.34205891 -0.1431742  -0.16384015 -0.07394486]\n",
      "c:  28, W: [ 0.16       -0.31557991 -0.2445482  -0.15053015 -0.01923786]\n",
      "c:  27, W: [ 0.17       -0.30107891 -0.2084812  -0.19108715 -0.03520386]\n",
      "c:   2, W: [ 0.18       -0.27913591 -0.1629782  -0.24084715 -0.06245786]\n",
      "c:  23, W: [ 0.19       -0.31935391 -0.2460182  -0.11529715 -0.07755686]\n",
      "c:  11, W: [ 0.2        -0.31334891 -0.2266912  -0.14818515 -0.08079836]\n",
      "c:  11, W: [ 0.21       -0.30279691 -0.2148342  -0.17459615 -0.07969506]\n",
      "c:  78, W: [ 0.22       -0.28790091 -0.1805462  -0.21490515 -0.09395406]\n",
      "c:   6, W: [ 0.21       -0.29119331 -0.1359942  -0.26062315 -0.08406606]\n",
      "c:  38, W: [ 0.22       -0.30829731 -0.1837742  -0.19851415 -0.08009206]\n",
      "c:  13, W: [ 0.21       -0.31349231 -0.1511412  -0.22940915 -0.07024306]\n",
      "c:   2, W: [ 0.22       -0.34491531 -0.2815062  -0.07263615 -0.07685956]\n",
      "c:   6, W: [ 0.21       -0.36217231 -0.2368092  -0.15485515 -0.05878656]\n",
      "c:  25, W: [ 0.2        -0.37146931 -0.1988382  -0.20128415 -0.05582956]\n",
      "c:  20, W: [ 0.21       -0.35467031 -0.1567702  -0.24668215 -0.07976056]\n",
      "c:  23, W: [ 0.22       -0.38697531 -0.2289052  -0.13024915 -0.08922186]\n",
      "c:  10, W: [ 0.21       -0.38052811 -0.1828432  -0.21371915 -0.06212286]\n",
      "c:  74, W: [ 0.22       -0.36701011 -0.1722482  -0.23715615 -0.05812306]\n",
      "c:   5, W: [ 0.21       -0.35914111 -0.2679112  -0.19928915  0.01691094]\n",
      "c:  58, W: [ 0.22       -0.34406411 -0.2483152  -0.22987315  0.01568664]\n",
      "c:  26, W: [ 0.23       -0.33724611 -0.1998112  -0.28200615 -0.04535636]\n",
      "c:  38, W: [ 0.24       -0.36261911 -0.2694012  -0.19395215 -0.03006736]\n",
      "c:  53, W: [ 0.25       -0.33870211 -0.2238362  -0.24384015 -0.05905436]\n",
      "c:  22, W: [ 0.24       -0.34389711 -0.1912032  -0.27473515 -0.04920536]\n",
      "c:  12, W: [ 0.25       -0.38085811 -0.3279822  -0.09894015 -0.07538636]\n",
      "c:   1, W: [ 0.26       -0.37190691 -0.2802442  -0.14737115 -0.13129536]\n",
      "c: 113, W: [ 0.25       -0.37710161 -0.2476112  -0.17826615 -0.12144616]\n",
      "c:   1, W: [ 0.24       -0.38039361 -0.2030592  -0.22398415 -0.11155816]\n",
      "c:  38, W: [ 0.25       -0.36008361 -0.1845392  -0.25410515 -0.11152813]\n",
      "c:  49, W: [ 0.26       -0.33990661 -0.1665572  -0.28368615 -0.10942913]\n",
      "c: 114, W: [ 0.27       -0.34711341 -0.2341402  -0.22527815 -0.10319223]\n",
      "c:   2, W: [ 0.28       -0.32517041 -0.1886372  -0.27503815 -0.13044623]\n",
      "c:  34, W: [ 0.29       -0.36538841 -0.2716772  -0.14948815 -0.14554523]\n",
      "c:  11, W: [ 0.3        -0.35483641 -0.2598202  -0.17589915 -0.14444193]\n",
      "c:  63, W: [ 0.31       -0.33994041 -0.2255322  -0.21620815 -0.15870093]\n",
      "c:  15, W: [ 0.3        -0.31346141 -0.3269062  -0.20289815 -0.10399393]\n",
      "c:  44, W: [ 0.29       -0.31675381 -0.2823542  -0.24861615 -0.09410593]\n",
      "c:  49, W: [ 0.28       -0.32194881 -0.2497212  -0.27951115 -0.08425693]\n",
      "c:  17, W: [ 0.27       -0.32714381 -0.2170882  -0.31040615 -0.07440793]\n",
      "c:  23, W: [ 0.28       -0.35944881 -0.2892232  -0.19397315 -0.08386923]\n",
      "c: 211, W: [ 0.27       -0.35300161 -0.2431612  -0.27744315 -0.05677023]\n",
      "c:  11, W: [ 0.28       -0.32908461 -0.1975962  -0.32733115 -0.08575723]\n",
      "c:  37, W: [ 0.29       -0.36665761 -0.2805122  -0.22429915 -0.08195133]\n",
      "c:  27, W: [ 0.28       -0.36504681 -0.2158882  -0.30787215 -0.06673533]\n",
      "c:  12, W: [ 0.29       -0.40200781 -0.3526672  -0.13207715 -0.09291633]\n",
      "c:   1, W: [ 0.3        -0.39305661 -0.3049292  -0.18050815 -0.14882533]\n",
      "c: 113, W: [ 0.29       -0.39825131 -0.2722962  -0.21140315 -0.13897613]\n",
      "c:   1, W: [ 0.28       -0.40154331 -0.2277442  -0.25712115 -0.12908813]\n",
      "c:  38, W: [ 0.29       -0.38123331 -0.2092242  -0.28724215 -0.1290581 ]\n",
      "c:  49, W: [ 0.3        -0.36105631 -0.1912422  -0.31682315 -0.1269591 ]\n",
      "c:   6, W: [ 0.31       -0.36826311 -0.2588252  -0.25841515 -0.1207222 ]\n",
      "c:  29, W: [ 0.3        -0.36039411 -0.3544882  -0.22054815 -0.0456882 ]\n",
      "c:  52, W: [ 0.31       -0.42375811 -0.2616402  -0.2204054  -0.1135322 ]\n",
      "c:   3, W: [ 0.32       -0.40925711 -0.2255732  -0.2609624  -0.1294982 ]\n",
      "c:  24, W: [ 0.31       -0.38277811 -0.3269472  -0.2476524  -0.0747912 ]\n",
      "c: 125, W: [ 0.32       -0.36083511 -0.2814442  -0.2974124  -0.1020452 ]\n",
      "c:   6, W: [ 0.31       -0.36412751 -0.2368922  -0.3431304  -0.0921572 ]\n",
      "c:  38, W: [ 0.32       -0.38123151 -0.2846722  -0.2810214  -0.0881832 ]\n",
      "c:  13, W: [ 0.31       -0.38642651 -0.2520392  -0.3119164  -0.0783342 ]\n",
      "c:   2, W: [ 0.32       -0.41784951 -0.3824042  -0.1551434  -0.0849507 ]\n",
      "c:   6, W: [ 0.31       -0.43510651 -0.3377072  -0.2373624  -0.0668777 ]\n",
      "c:  25, W: [ 0.3        -0.44440351 -0.2997362  -0.2837914  -0.0639207 ]\n",
      "c:  20, W: [ 0.31       -0.42760451 -0.2576682  -0.3291894  -0.0878517 ]\n",
      "c:  23, W: [ 0.32       -0.45990951 -0.3298032  -0.2127564  -0.097313  ]\n",
      "c:  49, W: [ 0.31       -0.45346231 -0.2837412  -0.2962264  -0.070214  ]\n",
      "c:  35, W: [ 0.32       -0.43613131 -0.2441972  -0.3436384  -0.095231  ]\n",
      "c:   5, W: [ 0.31       -0.42826231 -0.3398602  -0.3057714  -0.020197  ]\n",
      "c: 122, W: [ 0.32       -0.41318531 -0.3202642  -0.3363554  -0.0214213 ]\n",
      "c:  21, W: [ 0.33       -0.38926831 -0.2746992  -0.3862434  -0.0504083 ]\n",
      "c:  27, W: [ 0.34       -0.42532131 -0.3344392  -0.2853274  -0.0586929 ]\n",
      "c:  27, W: [ 0.33       -0.42371051 -0.2698152  -0.3689004  -0.0434769 ]\n",
      "c:  12, W: [ 0.34       -0.46067151 -0.4065942  -0.1931054  -0.0696579 ]\n",
      "c:   1, W: [ 0.35       -0.45172031 -0.3588562  -0.2415364  -0.1255669 ]\n",
      "c: 113, W: [ 0.34       -0.45691501 -0.3262232  -0.2724314  -0.1157177 ]\n",
      "c:   1, W: [ 0.33       -0.46020701 -0.2816712  -0.3181494  -0.1058297 ]\n",
      "c:  38, W: [ 0.34       -0.43989701 -0.2631512  -0.3482704  -0.10579967]\n",
      "c:  52, W: [ 0.35       -0.41972001 -0.2451692  -0.3778514  -0.10370067]\n",
      "c:  84, W: [ 0.36       -0.43811101 -0.3360522  -0.2854354  -0.10474387]\n",
      "c: 152, W: [ 0.37       -0.42361001 -0.2999852  -0.3259924  -0.12070987]\n",
      "c:   6, W: [ 0.36       -0.42690241 -0.2554332  -0.3717104  -0.11082187]\n",
      "c:  38, W: [ 0.37       -0.44400641 -0.3032132  -0.3096014  -0.10684787]\n",
      "c:  49, W: [ 0.36       -0.44920141 -0.2705802  -0.3404964  -0.09699887]\n",
      "c:  17, W: [ 0.35       -0.45439641 -0.2379472  -0.3713914  -0.08714987]\n",
      "c:  23, W: [ 0.36       -0.48670141 -0.3100822  -0.2549584  -0.09661117]\n",
      "c:  84, W: [ 0.35       -0.48025421 -0.2640202  -0.3384284  -0.06951217]\n",
      "c:   5, W: [ 0.34       -0.47238521 -0.3596832  -0.3005614   0.00552183]\n",
      "c:  82, W: [ 0.35       -0.45730821 -0.3400872  -0.3311454   0.00429753]\n",
      "c:  38, W: [ 0.36       -0.44993221 -0.2915622  -0.3791314  -0.05236147]\n",
      "c:   2, W: [ 0.37       -0.49370521 -0.3467292  -0.2697414  -0.05644347]\n",
      "c:  53, W: [ 0.38       -0.46978821 -0.3011642  -0.3196294  -0.08543047]\n",
      "c:  22, W: [ 0.37       -0.47498321 -0.2685312  -0.3505244  -0.07558147]\n",
      "c:  12, W: [ 0.38       -0.51194421 -0.4053102  -0.1747294  -0.10176247]\n",
      "c:   1, W: [ 0.39       -0.50299301 -0.3575722  -0.2231604  -0.15767147]\n",
      "c: 113, W: [ 0.38       -0.50818771 -0.3249392  -0.2540554  -0.14782227]\n",
      "c:   1, W: [ 0.37       -0.51147971 -0.2803872  -0.2997734  -0.13793427]\n",
      "c:  38, W: [ 0.38       -0.49116971 -0.2618672  -0.3298944  -0.13790424]\n",
      "c:  55, W: [ 0.39       -0.47099271 -0.2438852  -0.3594754  -0.13580524]\n",
      "c:  81, W: [ 0.38       -0.46312371 -0.3395482  -0.3216084  -0.06077124]\n",
      "c:  61, W: [ 0.39       -0.44862271 -0.3034812  -0.3621654  -0.07673724]\n",
      "c:  13, W: [ 0.4        -0.48220471 -0.3758852  -0.2477464  -0.08244854]\n",
      "c:  78, W: [ 0.41       -0.46730871 -0.3415972  -0.2880554  -0.09670754]\n",
      "c:  44, W: [ 0.4        -0.47060111 -0.2970452  -0.3337734  -0.08681954]\n",
      "c:  13, W: [ 0.39       -0.47579611 -0.2644122  -0.3646684  -0.07697054]\n",
      "c:   8, W: [ 0.4        -0.50721911 -0.3947772  -0.2078954  -0.08358704]\n",
      "c:  25, W: [ 0.39       -0.51651611 -0.3568062  -0.2543244  -0.08063004]\n",
      "c:   3, W: [ 0.4        -0.49971711 -0.3147382  -0.2997224  -0.10456104]\n",
      "c: 124, W: [ 0.39       -0.50491211 -0.2821052  -0.3306174  -0.09471204]\n",
      "c:   5, W: [ 0.38       -0.49704311 -0.3777682  -0.2927504  -0.01967804]\n",
      "c:  82, W: [ 0.39       -0.48196611 -0.3581722  -0.3233344  -0.02090234]\n",
      "c:  40, W: [ 0.4        -0.47459011 -0.3096472  -0.3713204  -0.07756134]\n",
      "c:  11, W: [ 0.41       -0.45067311 -0.2640822  -0.4212084  -0.10654834]\n",
      "c:  37, W: [ 0.42       -0.48824611 -0.3469982  -0.3181764  -0.10274244]\n",
      "c:  27, W: [ 0.41       -0.48663531 -0.2823742  -0.4017494  -0.08752644]\n",
      "c:  12, W: [ 0.42       -0.52359631 -0.4191532  -0.2259544  -0.11370744]\n",
      "c:   1, W: [ 0.43       -0.51464511 -0.3714152  -0.2743854  -0.16961644]\n",
      "c: 113, W: [ 0.42       -0.51983981 -0.3387822  -0.3052804  -0.15976724]\n",
      "c:   1, W: [ 0.41       -0.52313181 -0.2942302  -0.3509984  -0.14987924]\n",
      "c:  93, W: [ 0.42       -0.50282181 -0.2757102  -0.3811194  -0.14984921]\n",
      "c:  81, W: [ 0.41       -0.49495281 -0.3713732  -0.3432524  -0.07481521]\n",
      "c:  27, W: [ 0.42       -0.48045181 -0.3353062  -0.3838094  -0.09078121]\n",
      "c:   2, W: [ 0.43       -0.45850881 -0.2898032  -0.4335694  -0.11803521]\n",
      "c:  45, W: [ 0.44       -0.49872681 -0.3728432  -0.3080194  -0.13313421]\n",
      "c:  78, W: [ 0.45       -0.48383081 -0.3385552  -0.3483284  -0.14739321]\n",
      "c:  44, W: [ 0.44       -0.48712321 -0.2940032  -0.3940464  -0.13750521]\n",
      "c:  13, W: [ 0.43       -0.49231821 -0.2613702  -0.4249414  -0.12765621]\n",
      "c:   8, W: [ 0.44       -0.52374121 -0.3917352  -0.2681684  -0.13427271]\n",
      "c:  25, W: [ 0.43       -0.53303821 -0.3537642  -0.3145974  -0.13131571]\n",
      "c:   3, W: [ 0.44       -0.51623921 -0.3116962  -0.3599954  -0.15524671]\n",
      "c:  17, W: [ 0.43       -0.52143421 -0.2790632  -0.3908904  -0.14539771]\n",
      "c:  23, W: [ 0.44       -0.55373921 -0.3511982  -0.2744574  -0.15485901]\n",
      "c:  84, W: [ 0.43       -0.54729201 -0.3051362  -0.3579274  -0.12776001]\n",
      "c:   5, W: [ 0.42       -0.53942301 -0.4007992  -0.3200604  -0.05272601]\n",
      "c: 122, W: [ 0.43       -0.52434601 -0.3812032  -0.3506444  -0.05395031]\n",
      "c:  53, W: [ 0.44       -0.50042901 -0.3356382  -0.4005324  -0.08293731]\n",
      "c:  22, W: [ 0.43       -0.50562401 -0.3030052  -0.4314274  -0.07308831]\n",
      "c:  12, W: [ 0.44       -0.54258501 -0.4397842  -0.2556324  -0.09926931]\n",
      "c:   1, W: [ 0.45       -0.53363381 -0.3920462  -0.3040634  -0.15517831]\n",
      "c: 113, W: [ 0.44       -0.53882851 -0.3594132  -0.3349584  -0.14532911]\n",
      "c:   1, W: [ 0.43       -0.54212051 -0.3148612  -0.3806764  -0.13544111]\n",
      "c:  93, W: [ 0.44       -0.52181051 -0.2963412  -0.4107974  -0.13541108]\n",
      "c:  81, W: [ 0.43       -0.51394151 -0.3920042  -0.3729304  -0.06037708]\n",
      "c:  27, W: [ 0.44       -0.49944051 -0.3559372  -0.4134874  -0.07634308]\n",
      "c:   2, W: [ 0.45       -0.47749751 -0.3104342  -0.4632474  -0.10359708]\n",
      "c:  45, W: [ 0.46       -0.51771551 -0.3934742  -0.3376974  -0.11869608]\n",
      "c:  78, W: [ 0.47       -0.50281951 -0.3591862  -0.3780064  -0.13295508]\n",
      "c:  44, W: [ 0.46       -0.50611191 -0.3146342  -0.4237244  -0.12306708]\n",
      "c:  13, W: [ 0.45       -0.51130691 -0.2820012  -0.4546194  -0.11321808]\n",
      "c:   8, W: [ 0.46       -0.54272991 -0.4123662  -0.2978464  -0.11983458]\n",
      "c:  25, W: [ 0.45       -0.55202691 -0.3743952  -0.3442754  -0.11687758]\n",
      "c:   3, W: [ 0.46       -0.53522791 -0.3323272  -0.3896734  -0.14080858]\n",
      "c:  17, W: [ 0.45       -0.54042291 -0.2996942  -0.4205684  -0.13095958]\n",
      "c:  23, W: [ 0.46       -0.57272791 -0.3718292  -0.3041354  -0.14042088]\n",
      "c:  84, W: [ 0.45       -0.56628071 -0.3257672  -0.3876054  -0.11332188]\n",
      "c:   5, W: [ 0.44       -0.55841171 -0.4214302  -0.3497384  -0.03828788]\n",
      "c: 122, W: [ 0.45       -0.54333471 -0.4018342  -0.3803224  -0.03951218]\n",
      "c:  32, W: [ 0.46       -0.51941771 -0.3562692  -0.4302104  -0.06849918]\n",
      "c:  16, W: [ 0.47       -0.55028371 -0.4226312  -0.3248054  -0.07741738]\n",
      "c:   5, W: [ 0.46       -0.54867291 -0.3580072  -0.4083784  -0.06220138]\n",
      "c:  22, W: [ 0.45       -0.55386791 -0.3253742  -0.4392734  -0.05235238]\n",
      "c:  12, W: [ 0.46       -0.59082891 -0.4621532  -0.2634784  -0.07853338]\n",
      "c:   1, W: [ 0.47       -0.58187771 -0.4144152  -0.3119094  -0.13444238]\n",
      "c: 113, W: [ 0.46       -0.58707241 -0.3817822  -0.3428044  -0.12459318]\n",
      "c:   1, W: [ 0.45       -0.59036441 -0.3372302  -0.3885224  -0.11470518]\n",
      "c:  38, W: [ 0.46       -0.57005441 -0.3187102  -0.4186434  -0.11467515]\n",
      "c:  55, W: [ 0.47       -0.54987741 -0.3007282  -0.4482244  -0.11257615]\n",
      "c:  81, W: [ 0.46       -0.54200841 -0.3963912  -0.4103574  -0.03754215]\n",
      "c:  34, W: [ 0.47       -0.52750741 -0.3603242  -0.4509144  -0.05350815]\n",
      "c:   1, W: [ 0.48       -0.56571041 -0.4908752  -0.2813314  -0.07656015]\n",
      "c:  39, W: [ 0.49       -0.62051841 -0.4090562  -0.2785496  -0.12688315]\n",
      "c:  78, W: [ 0.5        -0.60562241 -0.3747682  -0.3188586  -0.14114215]\n",
      "c:  44, W: [ 0.49       -0.60891481 -0.3302162  -0.3645766  -0.13125415]\n",
      "c:  49, W: [ 0.48       -0.61410981 -0.2975832  -0.3954716  -0.12140515]\n",
      "c:  17, W: [ 0.47       -0.61930481 -0.2649502  -0.4263666  -0.11155615]\n",
      "c:  23, W: [ 0.48       -0.65160981 -0.3370852  -0.3099336  -0.12101745]\n",
      "c:  84, W: [ 0.47       -0.64516261 -0.2910232  -0.3934036  -0.09391845]\n",
      "c:   5, W: [ 0.46       -0.63729361 -0.3866862  -0.3555366  -0.01888445]\n",
      "c: 122, W: [ 0.47       -0.62221661 -0.3670902  -0.3861206  -0.02010875]\n",
      "c:  32, W: [ 0.48       -0.59829961 -0.3215252  -0.4360086  -0.04909575]\n",
      "c:  16, W: [ 0.49       -0.62916561 -0.3878872  -0.3306036  -0.05801395]\n",
      "c: 154, W: [ 0.48       -0.62755481 -0.3232632  -0.4141766  -0.04279795]\n",
      "c:  15, W: [ 0.49       -0.60724481 -0.3047432  -0.4442976  -0.04276792]\n",
      "c:  23, W: [ 0.5        -0.64741781 -0.3878662  -0.3197506  -0.05714292]\n",
      "c: 136, W: [ 0.51       -0.62724081 -0.3698842  -0.3493316  -0.05504392]\n",
      "c:  27, W: [ 0.52       -0.61273981 -0.3338172  -0.3898886  -0.07100992]\n",
      "c:   2, W: [ 0.53       -0.59079681 -0.2883142  -0.4396486  -0.09826392]\n",
      "c:  45, W: [ 0.54       -0.63101481 -0.3713542  -0.3140986  -0.11336292]\n",
      "c:  78, W: [ 0.55       -0.61611881 -0.3370662  -0.3544076  -0.12762192]\n",
      "c:  44, W: [ 0.54       -0.61941121 -0.2925142  -0.4001256  -0.11773392]\n",
      "c:  13, W: [ 0.53       -0.62460621 -0.2598812  -0.4310206  -0.10788492]\n",
      "c:   8, W: [ 0.54       -0.65602921 -0.3902462  -0.2742476  -0.11450142]\n",
      "c:  25, W: [ 0.53       -0.66532621 -0.3522752  -0.3206766  -0.11154442]\n",
      "c:   3, W: [ 0.54       -0.64852721 -0.3102072  -0.3660746  -0.13547542]\n",
      "c: 124, W: [ 0.53       -0.65372221 -0.2775742  -0.3969696  -0.12562642]\n",
      "c:   5, W: [ 0.52       -0.64585321 -0.3732372  -0.3591026  -0.05059242]\n",
      "c: 122, W: [ 0.53       -0.63077621 -0.3536412  -0.3896866  -0.05181672]\n",
      "c:  32, W: [ 0.54       -0.60685921 -0.3080762  -0.4395746  -0.08080372]\n",
      "c:  16, W: [ 0.55       -0.63772521 -0.3744382  -0.3341696  -0.08972192]\n",
      "c:   5, W: [ 0.54       -0.63611441 -0.3098142  -0.4177426  -0.07450592]\n",
      "c:  22, W: [ 0.53       -0.64130941 -0.2771812  -0.4486376  -0.06465692]\n",
      "c:  12, W: [ 0.54       -0.67827041 -0.4139602  -0.2728426  -0.09083792]\n",
      "c:   1, W: [ 0.55       -0.66931921 -0.3662222  -0.3212736  -0.14674692]\n",
      "c:  31, W: [ 0.54       -0.67451391 -0.3335892  -0.3521686  -0.13689772]\n",
      "c:  82, W: [ 0.53       -0.65592991 -0.4124492  -0.3355256  -0.11851372]\n",
      "c:   1, W: [ 0.52       -0.65922191 -0.3678972  -0.3812436  -0.10862572]\n",
      "c:  38, W: [ 0.53       -0.63891191 -0.3493772  -0.4113646  -0.10859569]\n",
      "c:  55, W: [ 0.54       -0.61873491 -0.3313952  -0.4409456  -0.10649669]\n",
      "c:  36, W: [ 0.53       -0.61086591 -0.4270582  -0.4030786  -0.03146269]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(clf.w_list)):\n",
    "    print(\"c: {:3}, W: {}\".format(int(clf.c_list[i]), clf.w_list[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) [16 points] Implement the average Perceptron. Set the maximum number of\n",
    "epochs T to 10. Report your learned weight vector. Comparing with the list of\n",
    "weight vectors from (b), what can you observe? Report the average prediction\n",
    "error on the test data.\n",
    "\n",
    "Comparing the list of weight vectors from (b), the voted perceptron return a list of all the different weight vectors learned while the average perceptron essentially return the same weight vectors but it is combined into one weight vector. \n",
    "\n",
    "vote 5.6\n",
    "average 5.8\n",
    "stand 6.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Perceptron Error: 0.312\n",
      "Average Perceptron W: [  -39933.2         -781379.29113256 -1060703.99293281   174290.29208337\n",
      "    42352.04815224]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_training = np.genfromtxt('../data/bank-note/train.csv', delimiter=',')\n",
    "X_test = np.genfromtxt('../data/bank-note/test.csv', delimiter=',')\n",
    "y_training = X_training[:, -1]\n",
    "y_test = X_test[:, -1]\n",
    "y_training[y_training == 0] = -1\n",
    "y_test[y_test == 0] = -1\n",
    "\n",
    "X_training = np.insert(X_training, 0, 1, axis=1)\n",
    "X_test = np.insert(X_test, 0, 1, axis=1)\n",
    "\n",
    "X_training = X_training[:, :-1]\n",
    "X_test = X_test[:, :-1]\n",
    "\n",
    "clf = Perceptron(variant='average', epoch=10, r=0.01)\n",
    "clf.fit(X_training, y_training)\n",
    "y_predicted = clf.predict(X_test)\n",
    "\n",
    "error = 0\n",
    "for i in range(0, len(X_test)):\n",
    "    if y_predicted[i] != y_test[i]:\n",
    "        error += 1\n",
    "error = error / len(X_test)\n",
    "print(\"Average Perceptron Error: \" + str(error))\n",
    "print(\"Average Perceptron W: \" + str(clf.w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) [14 points] Compare the average prediction errors for the three methods. What\n",
    "do you conclude?\n",
    "\n",
    "standard error: 0.066\n",
    "average error: 0.014\n",
    "\n",
    "The average prediction errors for the three methods are very close. The standard Perceptron has an average prediction error of 0.064, while both the voted Perceptron and the average Perceptron exhibit an average prediction error of 0.056. The average prediction error for the voted Perceptron and the average Perceptron is the same because the average Perceptron is essentially the voted Perceptron, with weights and associated votes implicitly created by the alpha vector. However, the average prediction error for the standard Perceptron fluctuates when compared to the others due to the shuffling of the data. This makes the predicted weights highly sensitive to the data order."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
