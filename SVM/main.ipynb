{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. [28 points] We will first implement SVM in the primal domain with stochastic subgradient descent. We will reuse the dataset for Perceptron implementation, namely, “bank-note.zip” in Canvas. The features and labels are listed in the file “classification/data-desc.txt”. The training data are stored in the file “classification/train.csv”, consisting of 872 examples. The test data are stored in “classification/test.csv”, and comprise of 500 examples. In both the training and test datasets, feature values and labels are separated by commas. Set the maximum epochs T to 100. Don’t forget to shuffle the training examples at the start of each epoch. Use the curve of the objective function (along with the number of updates) to diagnosis the convergence. Try the hyperparameter C from {100/873, 500/873, 700/873}. Don’t forget to convert the labels to be in {1,−1}.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from svm import SVM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X_training = np.genfromtxt('../data/bank-note/train.csv', delimiter=',')\n",
    "X_test = np.genfromtxt('../data/bank-note/test.csv', delimiter=',')\n",
    "\n",
    "y_training = X_training[:, -1]\n",
    "y_test = X_test[:, -1]\n",
    "\n",
    "y_training[y_training == 0] = -1\n",
    "y_test[y_test == 0] = -1\n",
    "\n",
    "X_training = np.insert(X_training, 0, 1, axis=1)\n",
    "X_test = np.insert(X_test, 0, 1, axis=1)\n",
    "\n",
    "X_training = X_training[:, :-1]\n",
    "X_test = X_test[:, :-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) [12 points] Use the schedule of learning rate: γt = γ0 / 1+(γ0/a) * t. Please tune γ0 > 0 and\n",
    "a > 0 to ensure convergence. For each setting of C, report your training and test error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:  0.1145475372279496\n",
      "Training error:  0.033256880733944956\n",
      "Test error:  0.04\n",
      "C:  0.572737686139748\n",
      "Training error:  0.034403669724770644\n",
      "Test error:  0.036\n",
      "C:  0.8018327605956472\n",
      "Training error:  0.01834862385321101\n",
      "Test error:  0.024\n"
     ]
    }
   ],
   "source": [
    "C = [100/873, 500/873, 700/873]\n",
    "epoch = 100\n",
    "r = 0.01\n",
    "a = 0.01\n",
    "default_schedule = True\n",
    "train_errors_1 = []\n",
    "test_errors_1 = []\n",
    "models_1 = []\n",
    "\n",
    "for i in range(len(C)):\n",
    "    svm = SVM(variant='primal', C=C[i], epoch=epoch, r=r, a=a, default_schedule=default_schedule)\n",
    "    svm.fit(X_training, y_training)\n",
    "    train_pred_1 = svm.predict(X_training)\n",
    "    test_pred_1 = svm.predict(X_test)\n",
    "    train_error_1 = np.sum(train_pred_1 != y_training) / len(y_training)\n",
    "    test_error_1 = np.sum(test_pred_1 != y_test) / len(y_test)\n",
    "    train_errors_1.append(train_error_1)\n",
    "    test_errors_1.append(test_error_1)\n",
    "    models_1.append(svm.get_model())\n",
    "\n",
    "    print('C: ', C[i])\n",
    "    print('Training error: ', train_error_1)\n",
    "    print('Test error: ', test_error_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) [12 points] Use the schedule γt = γ0 / 1+t. Report the training and test error for each\n",
    "setting of C.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:  0.1145475372279496\n",
      "Training error:  0.0963302752293578\n",
      "Test error:  0.11\n",
      "C:  0.572737686139748\n",
      "Training error:  0.04128440366972477\n",
      "Test error:  0.034\n",
      "C:  0.8018327605956472\n",
      "Training error:  0.04128440366972477\n",
      "Test error:  0.034\n"
     ]
    }
   ],
   "source": [
    "C = [100/873, 500/873, 700/873]\n",
    "epoch = 100\n",
    "r = 0.01\n",
    "a = 0.01\n",
    "default_schedule = False\n",
    "train_errors_2 = []\n",
    "test_errors_2 = []\n",
    "models_2 = []\n",
    "\n",
    "for i in range(len(C)):\n",
    "    svm = SVM(variant='primal', C=C[i], epoch=epoch, r=r, a=a, default_schedule=default_schedule)\n",
    "    svm.fit(X_training, y_training)\n",
    "    train_pred_2 = svm.predict(X_training)\n",
    "    test_pred_2 = svm.predict(X_test)\n",
    "    train_error_2 = np.sum(train_pred_2 != y_training) / len(y_training)\n",
    "    test_error_2 = np.sum(test_pred_2 != y_test) / len(y_test)\n",
    "    train_errors_2.append(train_error_2)\n",
    "    test_errors_2.append(test_error_2)\n",
    "    models_2.append(svm.get_model())\n",
    "\n",
    "    print('C: ', C[i])\n",
    "    print('Training error: ', train_error_2)\n",
    "    print('Test error: ', test_error_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) [6 points] For each C, report the differences between the model parameters learned\n",
    "from the two learning rate schedules, as well as the differences between the train-\n",
    "ing/test errors. What can you conclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in training error between default and non-default schedule: \n",
      "C:  0.1145475372279496\n",
      "0.04357798165137614\n",
      "C:  0.572737686139748\n",
      "0.009174311926605505\n",
      "C:  0.8018327605956472\n",
      "0.0022935779816513763\n",
      "\n",
      "\n",
      "Difference in test error between default and non-default schedule: \n",
      "C:  0.1145475372279496\n",
      "0.036\n",
      "C:  0.572737686139748\n",
      "0.0020000000000000018\n",
      "C:  0.8018327605956472\n",
      "0.013999999999999999\n",
      "\n",
      "\n",
      "Difference in models between default and non-default schedule: \n",
      "C:  0.1145475372279496\n",
      "[0.09988545 0.52658672 0.81622496 1.20032015 0.35963174]\n",
      "C:  0.572737686139748\n",
      "[0.         1.50031196 0.31420218 3.5837427  0.32286674]\n",
      "C:  0.8018327605956472\n",
      "[0.         1.73442643 0.69604478 6.54326985 7.14443554]\n",
      "\n",
      "\n",
      "Models for default schedule: \n",
      "[ 2.99656357 -3.24264552 -1.88218553 -2.97874167 -0.98903095]\n",
      "[ 10.98739977 -14.60346041 -10.82765047  -8.89138346  -3.93749752]\n",
      "[ 16.08155785 -19.3894399  -13.95640795 -10.37500306  -1.64404283]\n",
      "Models for non-default schedule: \n",
      "[ 3.09644903 -3.76923224 -2.69841049 -1.77842153 -0.62939921]\n",
      "[ 10.98739977 -13.10314845 -11.14185265 -12.47512616  -4.26036426]\n",
      "[ 16.08155785 -21.12386633 -13.26036316 -16.91827291  -8.78847837]\n"
     ]
    }
   ],
   "source": [
    "print('Difference in training error between default and non-default schedule: ')\n",
    "for i in range(len(C)):\n",
    "    print('C: ', C[i])\n",
    "    print(np.abs(train_errors_1[i] - train_errors_2[i]))\n",
    "print('\\n')\n",
    "\n",
    "# print(np.abs(np.array(train_errors_1) - np.array(train_errors_2)), '\\n')\n",
    "print('Difference in test error between default and non-default schedule: ')\n",
    "for i in range(len(C)):\n",
    "    print('C: ', C[i])\n",
    "    print(np.abs(test_errors_1[i] - test_errors_2[i]))\n",
    "print('\\n')\n",
    "\n",
    "print('Difference in models between default and non-default schedule: ')\n",
    "for i in range(len(C)):\n",
    "    print('C: ', C[i])\n",
    "    print(np.abs(models_1[i] - models_2[i]))\n",
    "\n",
    "print('\\n')\n",
    "print('Models for default schedule: ')\n",
    "for i in range(len(models_1)):\n",
    "    print(models_1[i])\n",
    "print('Models for non-default schedule: ')\n",
    "for i in range(len(models_2)):\n",
    "    print(models_2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [30 points] Now let us implement SVM in the dual domain. We use the same dataset, “bank-note.zip”. You can utilize existing constrained optimization libraries. For Python, we recommend using “scipy.optimize.minimize”, and you can learn how to use this API from the document at https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.optimize.minimize.html. We recommend using SLSQP to incorporate the equality constraints. For Matlab, we recommend using the internal function “fmincon”; the document and examples are given at https://www.mathworks.com/help/optim/ug/fmincon.html. For R, we recommend using the “nloptr” package with detailed documentation at https://cran.r-project.org/web/packages/nloptr/nloptr.pdf.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) [10 points] First, run your dual SVM learning algorithm with C in {100 / 873, 500 / 873, 700 /873}. Recover the feature weights w and the bias b. Compare with the parameters learned with stochastic sub-gradient descent in the primal domain (in Problem 2) and the same settings of C, what can you observe? What do you conclude and why? Note that if your code calculates the objective function with a double loop, the optimization can be quite slow. To accelerate, consider writing down the objective in terms of the matrix and vector operations, and treat the Lagrange\n",
    "multipliers that we want to optimize as a vector! Recall, we have discussed about it in our class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.51728438 -0.94292749 -0.65149165 -0.73372163 -0.0410216 ]\n",
      "C:  0.1145475372279496\n",
      "Training error:  0.026376146788990827\n",
      "Test error:  0.03\n",
      "\n",
      "\n",
      "[ 3.96538542 -1.56393877 -1.01405238 -1.18065128 -0.15651755]\n",
      "C:  0.572737686139748\n",
      "Training error:  0.03096330275229358\n",
      "Test error:  0.036\n",
      "\n",
      "\n",
      "[ 5.0371374  -2.04255067 -1.28070426 -1.51352641 -0.24906657]\n",
      "C:  0.8018327605956472\n",
      "Training error:  0.034403669724770644\n",
      "Test error:  0.036\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from svm import SVM\n",
    "import numpy as np\n",
    "\n",
    "X_training = np.genfromtxt('../data/bank-note/train.csv', delimiter=',')\n",
    "X_test = np.genfromtxt('../data/bank-note/test.csv', delimiter=',')\n",
    "y_training = X_training[:, -1]\n",
    "y_test = X_test[:, -1]\n",
    "y_training[y_training == 0] = -1\n",
    "y_test[y_test == 0] = -1\n",
    "\n",
    "# X_training = np.insert(X_training, 0, 1, axis=1)\n",
    "# X_test = np.insert(X_test, 0, 1, axis=1)\n",
    "\n",
    "X_training = X_training[:, :-1]\n",
    "X_test = X_test[:, :-1]\n",
    "\n",
    "\n",
    "\n",
    "C_dual = [100/873, 500/873, 700/873]\n",
    "\n",
    "dual_train_errors_1 = []\n",
    "dual_test_errors_1 = []\n",
    "dual_models_1 = []\n",
    "\n",
    "for i in range(len(C_dual)):\n",
    "    svm_dual = SVM(variant='dual', C=C_dual[i])\n",
    "    svm_dual.fit(X_training, y_training)\n",
    "    print(svm_dual.get_model())\n",
    "\n",
    "    d_train_pred_1 = svm_dual.predict(X_training)\n",
    "    d_test_pred_1 = svm_dual.predict(X_test)\n",
    "\n",
    "    d_train_error_1 = np.sum(d_train_pred_1 != y_training) / len(y_training)\n",
    "    d_test_error_1 = np.sum(d_test_pred_1 != y_test) / len(y_test)\n",
    "\n",
    "    dual_train_errors_1.append(d_train_error_1)\n",
    "    dual_test_errors_1.append(d_test_error_1)\n",
    "\n",
    "    dual_models_1.append(svm_dual.get_model())\n",
    "\n",
    "    print('C: ', C_dual[i])\n",
    "    print('Training error: ', d_train_error_1)\n",
    "    print('Test error: ', d_test_error_1)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) [15 points] Now, use Gaussian kernel in the dual form to implement the non-\n",
    "linear SVM. Note that you need to modify both the objective function and the\n",
    "prediction. The Gaussian kernel is defined as follows:\n",
    "k(xi,xj) = exp(−‖xi −xj‖^2 / γ ). Test γ from {0.1,0.5,1,5,100} and the hyperparameter C from {100/\n",
    "873, 500/873, 700/873}. List the training and test errors for the combinations of all the γ and C values. What is the best combination? Compared with linear SVM with the same settings of C, what do you observe? What do you conclude and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:  0.1145475372279496 r:  0.1\n",
      "Training error:  0.04128440366972477\n",
      "Test error:  0.39\n",
      "\n",
      "\n",
      "C:  0.1145475372279496 r:  0.5\n",
      "Training error:  0.010321100917431193\n",
      "Test error:  0.118\n",
      "\n",
      "\n",
      "C:  0.1145475372279496 r:  1\n",
      "Training error:  0.0\n",
      "Test error:  0.02\n",
      "\n",
      "\n",
      "C:  0.1145475372279496 r:  5\n",
      "Training error:  0.0\n",
      "Test error:  0.0\n",
      "\n",
      "\n",
      "C:  0.1145475372279496 r:  100\n",
      "Training error:  0.021788990825688075\n",
      "Test error:  0.022\n",
      "\n",
      "\n",
      "C:  0.572737686139748 r:  0.1\n",
      "Training error:  0.0\n",
      "Test error:  0.2\n",
      "\n",
      "\n",
      "C:  0.572737686139748 r:  0.5\n",
      "Training error:  0.0\n",
      "Test error:  0.012\n",
      "\n",
      "\n",
      "C:  0.572737686139748 r:  1\n",
      "Training error:  0.0\n",
      "Test error:  0.004\n",
      "\n",
      "\n",
      "C:  0.572737686139748 r:  5\n",
      "Training error:  0.0\n",
      "Test error:  0.004\n",
      "\n",
      "\n",
      "C:  0.572737686139748 r:  100\n",
      "Training error:  0.010321100917431193\n",
      "Test error:  0.012\n",
      "\n",
      "\n",
      "C:  0.8018327605956472 r:  0.1\n",
      "Training error:  0.0\n",
      "Test error:  0.18\n",
      "\n",
      "\n",
      "C:  0.8018327605956472 r:  0.5\n",
      "Training error:  0.0\n",
      "Test error:  0.01\n",
      "\n",
      "\n",
      "C:  0.8018327605956472 r:  1\n",
      "Training error:  0.0\n",
      "Test error:  0.004\n",
      "\n",
      "\n",
      "C:  0.8018327605956472 r:  5\n",
      "Training error:  0.0\n",
      "Test error:  0.002\n",
      "\n",
      "\n",
      "C:  0.8018327605956472 r:  100\n",
      "Training error:  0.010321100917431193\n",
      "Test error:  0.012\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from svm import SVM\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "X_training = np.genfromtxt('../data/bank-note/train.csv', delimiter=',')\n",
    "X_test = np.genfromtxt('../data/bank-note/test.csv', delimiter=',')\n",
    "y_training = X_training[:, -1]\n",
    "y_test = X_test[:, -1]\n",
    "y_training[y_training == 0] = -1\n",
    "y_test[y_test == 0] = -1\n",
    "\n",
    "# X_training = np.insert(X_training, 0, 1, axis=1)\n",
    "# X_test = np.insert(X_test, 0, 1, axis=1)\n",
    "\n",
    "X_training = X_training[:, :-1]\n",
    "X_test = X_test[:, :-1]\n",
    "\n",
    "C = [100/873, 500/873, 700/873] # regularization parameter\n",
    "r = [0.1, 0.5, 1, 5, 100]\n",
    "\n",
    "dual_train_errors_2 = []\n",
    "dual_test_errors_2 = []\n",
    "dual_models_2 = []\n",
    "\n",
    "for i in range(len(C)):\n",
    "    for j in range(len(r)):\n",
    "        svm_dual = SVM(variant='dual', C=C[i], r=r[j], kernel='gaussian')\n",
    "        svm_dual.fit(X_training, y_training)\n",
    "\n",
    "        d_train_pred_2 = svm_dual.predict(X_training)\n",
    "        d_test_pred_2 = svm_dual.predict(X_test)\n",
    "\n",
    "        d_train_error_2 = np.sum(d_train_pred_2 != y_training) / len(y_training)\n",
    "        d_test_error_2 = np.sum(d_test_pred_2 != y_test) / len(y_test)\n",
    "\n",
    "        dual_train_errors_2.append(d_train_error_2)\n",
    "        dual_test_errors_2.append(d_test_error_2)\n",
    "\n",
    "        print('C: ', C[i], 'r: ', r[j])\n",
    "        print('Training error: ', d_train_error_2)\n",
    "        print('Test error: ', d_test_error_2)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) [5 points] Following (b), for each setting of γ and C, list the number of support\n",
    "vectors. When C = 500/873, report the number of overlapped support vectors between consecutive values of γ, i.e., how many support vectors are the same for γ = 0.01 and γ = 0.1; how many are the same for γ = 0.1 and γ = 0.5, etc. What do you observe and conclude? Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
